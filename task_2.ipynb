{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. [3 балла] Извлечение именованных сущностей\n",
    "1. Обучите стандартную модель для извлечения именованных сущностей, CNN-BiLSTM-CRF, для извлечения именованных *низкоуровневых именованных сущностей*, т.е. для самых коротких из вложенных сущностей. \n",
    "Модель устроена так: сверточная сеть на символах + эмбеддинги слов + двунаправленная LSTM сеть (модель последовательности) + CRF (глобальная нормализация).\n",
    "2. Замените часть модели на символах и словах (CNN + эмбеддинги словах) на ELMo и / или BERT. Должна получиться модель ELMo / BERT + BiLSTM + CRF. \n",
    "3. Замените модель последовательности (BiLSTM) на другой слой, например, на Transformer. Должна получиться модель CNN  + Transformer + CRF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import itertools as it\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Доработать по 1 заданию:__\n",
    "1. BiLSTM по батчам\n",
    "2. добавить CNN\n",
    "3. добавить предобученные вектора\n",
    "\n",
    "__2 задание:__\n",
    "1. Подставить BERT\\ELMo вместо представлений\n",
    "\n",
    "__3 задание:__\n",
    "1. Подставить Transformer вместо BiLSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 155.91it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 132.02it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 135.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tager.transform_tsv import load_data_from\n",
    "sent_train, sent_dev, sent_test = load_data_from(\"litbank/entities/tsv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words\n",
    "words = it.chain(sent_train, sent_dev, sent_test)\n",
    "words = it.chain(*map(itemgetter(0), words))\n",
    "ix_to_word = sorted(set(words))\n",
    "word_to_ix = dict(zip(ix_to_word, range(len(ix_to_word))))\n",
    "\n",
    "# chars\n",
    "ix_to_char = sorted(set(it.chain(*ix_to_word)))\n",
    "char_to_ix = dict(zip(ix_to_char, range(len(ix_to_char))))\n",
    "char_to_ix[\"<pad>\"] = len(char_to_ix)\n",
    "\n",
    "# tags\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "tags = it.chain(sent_train, sent_dev, sent_test)\n",
    "tags = it.chain(*map(itemgetter(1), tags))\n",
    "ix_to_tag = sorted(set(tags))+[START_TAG, STOP_TAG]\n",
    "tag_to_ix = dict(zip(ix_to_tag, range(len(ix_to_tag))))\n",
    "\n",
    "len(word_to_ix), len(char_to_ix), len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Load pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-30 14:07:01--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
      "--2019-10-30 14:07:02--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
      "--2019-10-30 14:07:03--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182753 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.03MB/s    in 6m 34s  \n",
      "\n",
      "2019-10-30 14:13:38 (2.08 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove_vectors/glove.6B.100d.txt  \n",
      "  inflating: glove_vectors/glove.6B.200d.txt  \n",
      "  inflating: glove_vectors/glove.6B.300d.txt  \n",
      "  inflating: glove_vectors/glove.6B.50d.txt  \n"
     ]
    }
   ],
   "source": [
    "! unzip glove.6B.zip -d glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embeds(word_to_ix, path_to_glove_file):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        words: set\n",
    "        path_to_glove_file: str\n",
    "    \n",
    "    Return:\n",
    "        dict: key - word, value - np.ndarray\n",
    "    \"\"\"\n",
    "    embeds_matrix = np.random.normal(size=(len(word_to_ix), 100))\n",
    "    \n",
    "    word_to_ix = dict((k.lower(), v) for k,v in word_to_ix.items())\n",
    "    \n",
    "    with open(path_to_glove_file) as f:\n",
    "        for line in f.readlines():\n",
    "            word, *vec = line.split()\n",
    "            if word in word_to_ix:\n",
    "                embeds_matrix[word_to_ix[word],:] = np.array(list(map(float, vec)))\n",
    "                \n",
    "    return embeds_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_matrix = load_word_embeds(word_to_ix, \"glove_vectors/glove.6B.100d.txt\")\n",
    "glove_embeds = nn.Embedding(len(word_to_ix), 100)\n",
    "glove_embeds.load_state_dict({'weight':torch.tensor(glove_word_matrix)})\n",
    "del glove_word_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsevolod/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class LitBankDataset(Dataset):\n",
    "    \"\"\"LitBank dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, path_to_tsv_files, file_indexes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path_to_tsv_files (string): Path to the tsv file location.\n",
    "        \"\"\"\n",
    "        self.path_to_tsv_files = path_to_tsv_files\n",
    "        \n",
    "        # parse files\n",
    "        books = sorted(os.listdir(path_to_tsv_files))\n",
    "        books = books[file_indexes[0]:file_indexes[1]]\n",
    "        \n",
    "        self.sentences = list()\n",
    "        for book in books:\n",
    "            self.sentences.extend(self.get_sent_and_tags_from_tsv(path_to_tsv_files + book))\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_sent_and_tags_from_tsv(full_file_name):\n",
    "        \"\"\" Функция преобразует разметку из формата tsv в формат [sentence: list, tags: list].\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(full_file_name, sep=\"\\t\",\n",
    "                         header=None, quoting=csv.QUOTE_NONE)\n",
    "        df = df.loc[:, :1]\n",
    "        df.columns = [\"word\", \"tag\"]\n",
    "\n",
    "        split_ix = df.index[(df[\"word\"] == \".\") & (df[\"tag\"] == \"O\")].tolist()  # sent split index\n",
    "        words = df[\"word\"].tolist()\n",
    "        tags = df[\"tag\"].tolist()\n",
    "\n",
    "        sent = [words[start + 1:end + 1] for start, end in zip([0] + split_ix, split_ix)]\n",
    "        tags = [tags[start + 1:end + 1] for start, end in zip([0] + split_ix, split_ix)]\n",
    "\n",
    "        return list(zip(sent, tags))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        return self.sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LitBankDataset(\"litbank/entities/tsv/\", file_indexes=(0,80))\n",
    "dataset_dev = LitBankDataset(\"litbank/entities/tsv/\", file_indexes=(80,90))\n",
    "dataset_test = LitBankDataset(\"litbank/entities/tsv/\", file_indexes=(90,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'In', 'Chancery', 'London', '.']\n",
      "['O', 'O', 'B-FAC', 'B-GPE', 'O']\n"
     ]
    }
   ],
   "source": [
    "for sent, tags in dataset_train:\n",
    "    print(sent)\n",
    "    print(tags)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tager.utils import argmax, prepare_sequence, log_sum_exp\n",
    "from tager.model import BaseBiLSTM_CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. CNN_BiLSTM_CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модификация архитектуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerCharCNN(nn.Module):\n",
    "    \"\"\" LayerCharCNN implements character-level convolutional 1D layer.\n",
    "    source: https://github.com/achernodub/targer/blob/master/src/layers/layer_char_cnn.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, char_embeddings_dim, filter_num, char_window_size, char_to_ix):\n",
    "        super().__init__()\n",
    "        self.char_embeddings_dim = char_embeddings_dim\n",
    "        self.char_cnn_filter_num = filter_num\n",
    "        self.char_window_size = char_window_size\n",
    "        self.output_dim = char_embeddings_dim * filter_num\n",
    "        self.char_to_ix = char_to_ix\n",
    "        \n",
    "        self.char_embeds = nn.Embedding(len(char_to_ix), char_embeddings_dim)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=char_embeddings_dim,\n",
    "                                out_channels=char_embeddings_dim * filter_num,\n",
    "                                kernel_size=char_window_size,\n",
    "                                groups=char_embeddings_dim)\n",
    "\n",
    "    def prepare_word(self, seq, max_word_len):\n",
    "        \"\"\"Фукнция преобразует набор токенов в тензор из их id.\"\"\"\n",
    "        seq = it.chain(seq, (\"<pad>\" for i in range(max_word_len-len(seq))))\n",
    "        idxs = [self.char_to_ix[ch] for ch in seq]\n",
    "        return torch.tensor(idxs, dtype=torch.long)\n",
    "        \n",
    "    def _get_char_embeds(self, sentence):\n",
    "        max_word_len = max(map(len, sentence))\n",
    "        char_idx = torch.cat([self.prepare_word(word, max_word_len).view(1, -1)\n",
    "                                 for word in sentence], dim=0)\n",
    "        embeds = self.char_embeds(char_idx)\n",
    "        \n",
    "        return embeds\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        char_embeddings_feature = self._get_char_embeds(sentence).transpose(2, 1)\n",
    "        char_embeddings_feature = self.conv1d(char_embeddings_feature)\n",
    "        max_pooling_out, _ = torch.max(char_embeddings_feature, dim=2)\n",
    "\n",
    "        return max_pooling_out  # shape: seq_len x filter_num*char_embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn = LayerCharCNN(char_embeddings_dim=100, filter_num=1, char_window_size=3, char_to_ix=char_to_ix)\n",
    "sent = [\"send\", \"letter\"]\n",
    "char_cnn(sent).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM_CRF(BaseBiLSTM_CRF):\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_to_ix, char_to_ix, embedding_dim, char_embedding_dim,\n",
    "                 hidden_dim, char_window_size, max_word_len, word_embeds):\n",
    "        \n",
    "        super().__init__(vocab_size, tag_to_ix, embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.char_cnn = LayerCharCNN(\n",
    "            char_embeddings_dim=char_embedding_dim,\n",
    "            filter_num=1,\n",
    "            char_window_size=char_window_size,\n",
    "            char_to_ix=char_to_ix\n",
    "        )\n",
    "\n",
    "        self.word_embeds = word_embeds\n",
    "        self.lstm = nn.LSTM(embedding_dim*2, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "        \n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \n",
    "        # words to idxs\n",
    "        word_idxs = prepare_sentence(sentence)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # Get word add char embedings.\n",
    "        words_embeds = self.word_embeds(word_idxs)\n",
    "        chars_embeds = self.char_cnn(sentence)\n",
    "        embeds = torch.cat((words_embeds, chars_embeds), dim=1).view(len(sentence), 1, -1)\n",
    "    \n",
    "        # Run BiLSTM.\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "\n",
    "        # Transfom embeds to tag space.\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "\n",
    "        return lstm_feats\n",
    "    \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.7945, grad_fn=<SelectBackward>), [1, 0])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = dict(\n",
    "    vocab_size = len(word_to_ix),\n",
    "    tag_to_ix = tag_to_ix,\n",
    "    char_to_ix = char_to_ix,\n",
    "    embedding_dim = 100,\n",
    "    char_embedding_dim = 100,\n",
    "    hidden_dim = 100,\n",
    "    char_window_size = 3,\n",
    "    max_word_len=max_word_len,\n",
    "    word_embeds=glove_embeds\n",
    ")\n",
    "\n",
    "model = CNN_BiLSTM_CRF(**conf)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "model([\"send\", \"letter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "prepare_sentence = partial(prepare_sequence, to_ix=word_to_ix)\n",
    "prepare_tags = partial(prepare_sequence, to_ix=tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b70bdd0d3246a4a6d03446048835ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6130), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: invalid memory size -- maybe an overflow? at /opt/conda/conda-bld/pytorch-cpu_1549632688322/work/aten/src/TH/THGeneral.cpp:188",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-3a4516c96e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Courses/nlp/hw02/tager/model.py\u001b[0m in \u001b[0;36mneg_log_likelihood\u001b[0;34m(self, sentence, tags)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lstm_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mforward_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_alg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mgold_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-209-0ca09dbd6efb>\u001b[0m in \u001b[0;36m_get_lstm_features\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Get word add char embedings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mwords_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mchars_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-68f13ab15558>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mchar_embeddings_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_char_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mchar_embeddings_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_embeddings_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mmax_pooling_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_embeddings_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 187\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: invalid memory size -- maybe an overflow? at /opt/conda/conda-bld/pytorch-cpu_1549632688322/work/aten/src/TH/THGeneral.cpp:188"
     ]
    }
   ],
   "source": [
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    for sentence, tags in tqdm_notebook(dataset_train):\n",
    "\n",
    "        model.zero_grad()\n",
    "        targets = prepare_tags(tags)\n",
    "        loss = model.neg_log_likelihood(sentence, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['First',\n",
       "  ',',\n",
       "  'however',\n",
       "  ',',\n",
       "  'she',\n",
       "  'waited',\n",
       "  'for',\n",
       "  'a',\n",
       "  'few',\n",
       "  'minutes',\n",
       "  'to',\n",
       "  'see',\n",
       "  'if',\n",
       "  'she',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'shrink',\n",
       "  'any',\n",
       "  'further',\n",
       "  ':',\n",
       "  'she',\n",
       "  'felt',\n",
       "  'a',\n",
       "  'little',\n",
       "  'nervous',\n",
       "  'about',\n",
       "  'this',\n",
       "  ';',\n",
       "  '‘',\n",
       "  'for',\n",
       "  'it',\n",
       "  'might',\n",
       "  'end',\n",
       "  ',',\n",
       "  'you',\n",
       "  'know',\n",
       "  ',',\n",
       "  '’',\n",
       "  'said',\n",
       "  'Alice',\n",
       "  'to',\n",
       "  'herself',\n",
       "  ',',\n",
       "  '‘',\n",
       "  'in',\n",
       "  'my',\n",
       "  'going',\n",
       "  'out',\n",
       "  'altogether',\n",
       "  ',',\n",
       "  'like',\n",
       "  'a',\n",
       "  'candle',\n",
       "  '.'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(260.5890), [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(sent_train[3][0], word_to_ix)\n",
    "    print(model(precheck_sent))\n",
    "# We got it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
